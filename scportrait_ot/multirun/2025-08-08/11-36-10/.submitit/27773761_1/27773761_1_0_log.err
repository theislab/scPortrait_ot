wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: allepalma (inverse-perturbation-models) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /lustre/groups/ml01/workspace/alessandro.palma/scportrait/experiements/wandb/run-20250808_113844-rax9r4f7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-serenity-15
wandb: ‚≠êÔ∏è View project at https://wandb.ai/inverse-perturbation-models/scportrait_ot_sweep_distance_healthy_cite
wandb: üöÄ View run at https://wandb.ai/inverse-perturbation-models/scportrait_ot_sweep_distance_healthy_cite/runs/rax9r4f7
You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
You are using a CUDA device ('NVIDIA A100-PCIE-40GB MIG 3g.20gb') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [MIG-b0a94014-449c-583c-8c16-b9e11db18f44]

  | Name      | Type               | Params | Mode 
---------------------------------------------------------
0 | v_mlp     | TimeConditionedMLP | 2.4 M  | train
1 | criterion | MSELoss            | 0      | train
---------------------------------------------------------
2.4 M     Trainable params
0         Non-trainable params
2.4 M     Total params
9.462     Total estimated model params size (MB)
8         Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
submitit WARNING (2025-08-08 12:32:25,813) - Bypassing signal SIGCONT
[2025-08-08T12:32:25.813] error: *** JOB 27773782 ON gpusrv39 CANCELLED AT 2025-08-08T12:32:25 DUE to SIGNAL Terminated ***
[2025-08-08T12:32:25.835] error: *** STEP 27773782.0 ON gpusrv39 CANCELLED AT 2025-08-08T12:32:25 DUE to SIGNAL Terminated ***
[rank: 0] Received SIGTERM: 15
Bypassing SIGTERM: 15
submitit WARNING (2025-08-08 12:32:25,846) - Bypassing signal SIGTERM
--- Logging error ---
Traceback (most recent call last):
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1104, in emit
    self.flush()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1084, in flush
    self.stream.flush()
RuntimeError: reentrant call inside <_io.BufferedWriter name='/ictstr01/home/icb/alessandro.palma/environment/scportrait_ot/src/multirun/2025-08-08/11-36-10/1/main.log'>
Call stack:
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/submitit/core/_submit.py", line 11, in <module>
    submitit_main()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/submitit/core/submission.py", line 76, in submitit_main
    process_job(args.folder)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/submitit/core/submission.py", line 55, in process_job
    result = delayed.result()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/submitit/core/utils.py", line 137, in result
    self._result = self.function(*self.args, **self.kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/hydra_plugins/hydra_submitit_launcher/submitit_launcher.py", line 71, in __call__
    return run_job(
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "/ictstr01/home/icb/alessandro.palma/environment/scportrait_ot/src/main.py", line 23, in train
    estimator.train()
  File "/ictstr01/home/icb/alessandro.palma/environment/scportrait_ot/src/experiment.py", line 86, in train
    self.trainer_generative.fit(
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 539, in fit
    call._call_and_handle_interrupt(
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py", line 47, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 575, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 982, in _run
    results = self._run_stage()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1026, in _run_stage
    self.fit_loop.run()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 455, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 150, in run
    self.advance(data_fetcher)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/loops/training_epoch_loop.py", line 282, in advance
    batch, _, __ = next(data_fetcher)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 134, in __next__
    batch = super().__next__()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/loops/fetchers.py", line 61, in __next__
    batch = next(self.iterator)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 341, in __next__
    out = next(self._iterator)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/utilities/combined_loader.py", line 78, in __next__
    out[i] = next(self.iterators[i])
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 708, in __next__
    data = self._next_data()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1458, in _next_data
    idx, data = self._get_data()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1420, in _get_data
    success, data = self._try_get_data()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 1251, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/torch/multiprocessing/reductions.py", line 541, in rebuild_storage_fd
    fd = df.detach()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/connection.py", line 752, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/multiprocessing/connection.py", line 379, in _recv
    chunk = read(handle, remaining)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/submitit/core/job_environment.py", line 196, in bypass
    self._logger.warning(f"Bypassing signal {signal.Signals(signum).name}")
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1489, in warning
    self._log(WARNING, msg, args, **kwargs)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1624, in _log
    self.handle(record)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1634, in handle
    self.callHandlers(record)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1696, in callHandlers
    hdlr.handle(record)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 968, in handle
    self.emit(record)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1218, in emit
    StreamHandler.emit(self, record)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1104, in emit
    self.flush()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/logging/__init__.py", line 1084, in flush
    self.stream.flush()
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/signal_connector.py", line 33, in __call__
    signal_handler(signum, frame)
  File "/home/icb/alessandro.palma/miniconda3/envs/sc_exp_design/lib/python3.10/site-packages/submitit/core/job_environment.py", line 196, in bypass
    self._logger.warning(f"Bypassing signal {signal.Signals(signum).name}")
Message: 'Bypassing signal SIGTERM'
Arguments: ()
